{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Feautures Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imorting Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import features\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/TrainingSet\") as fh:\n",
    "    data = json.load(fh)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"review_id\", \"hotel_name\", \"review\", \"polarity\", \"spam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "df[\"num_of_words\"] = np.nan\n",
    "df[\"avg_words_per_sent\"]=np.nan\n",
    "df[\"unique_words\"]=np.nan\n",
    "df[\"self_words\"]=np.nan\n",
    "df[\"brand\"]=np.nan\n",
    "df[\"avg_word_length\"]=np.nan\n",
    "df[\"connectors\"]=np.nan\n",
    "df[\"digits\"]=np.nan\n",
    "df[\"verbs_per_noun\"]=np.nan\n",
    "df[\"adj\"]=np.nan\n",
    "df[\"prep\"]=np.nan\n",
    "df[\"adverb\"]=np.nan\n",
    "#Iterating through whole data-set to add linguistic features\n",
    "for review in df[\"review\"]:\n",
    "    a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12 = features.majorfunc(review)\n",
    "    df.iloc[iterator, df.columns.get_loc('num_of_words')] = a1\n",
    "    df.iloc[iterator, df.columns.get_loc('avg_words_per_sent')] = a2\n",
    "    df.iloc[iterator, df.columns.get_loc('unique_words')] = a3\n",
    "    df.iloc[iterator, df.columns.get_loc('self_words')] = a4\n",
    "    df.iloc[iterator, df.columns.get_loc('brand')] = a5\n",
    "    df.iloc[iterator, df.columns.get_loc('avg_word_length')] = a6\n",
    "    df.iloc[iterator, df.columns.get_loc('connectors')] = a7\n",
    "    df.iloc[iterator, df.columns.get_loc('digits')] = a8\n",
    "    df.iloc[iterator, df.columns.get_loc('verbs_per_noun')] = a9\n",
    "    df.iloc[iterator, df.columns.get_loc('adj')] = a10\n",
    "    df.iloc[iterator, df.columns.get_loc('prep')] = a11\n",
    "    df.iloc[iterator, df.columns.get_loc('adverb')] = a12\n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['hotel_name'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing useless column (eg. Review ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['review_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resulting DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['review'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   polarity  spam  num_of_words  avg_words_per_sent  unique_words  self_words  \\\n",
      "0         1     1          73.0           10.571429          53.0    0.073171   \n",
      "1         1     1         351.0           23.466667         181.0    0.074380   \n",
      "2         1     1         135.0           17.000000          94.0    0.046667   \n",
      "3         1     1         104.0            8.750000          61.0    0.058824   \n",
      "4         1     1         114.0           14.375000          80.0    0.072581   \n",
      "\n",
      "   brand  avg_word_length  connectors  digits  verbs_per_noun   adj  prep  \\\n",
      "0    0.0         4.451220         3.0     4.0             1.0   9.0   9.0   \n",
      "1    0.0         3.730028        12.0    10.0             1.0  30.0  54.0   \n",
      "2    0.0         4.066667         4.0     0.0             1.0  11.0  11.0   \n",
      "3    0.0         3.781513         5.0     0.0             1.0  10.0  12.0   \n",
      "4    0.0         4.145161         3.0     0.0             1.0   6.0  14.0   \n",
      "\n",
      "   adverb  \n",
      "0     3.0  \n",
      "1    28.0  \n",
      "2    12.0  \n",
      "3     9.0  \n",
      "4     6.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Function To word Tokenize and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mess):\n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dividing DataSet to TrainSet and TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 13) (396, 13) (1584,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(396, 13)\n",
      "      polarity  num_of_words  avg_words_per_sent  unique_words  self_words  \\\n",
      "1594        -1         376.0           11.424242         192.0    0.055156   \n",
      "1759        -1         119.0           17.142857          72.0    0.038760   \n",
      "713          1          73.0           10.571429          53.0    0.073171   \n",
      "108          1          54.0            9.166667          35.0    0.066667   \n",
      "348          1         121.0           17.428571          72.0    0.053846   \n",
      "87           1         123.0           17.714286          88.0    0.036496   \n",
      "1283        -1         250.0           15.687500         153.0    0.043165   \n",
      "700          1         118.0           11.900000          74.0    0.044776   \n",
      "1250        -1         128.0           14.333333          94.0    0.062500   \n",
      "223          1          39.0            8.000000          33.0    0.045455   \n",
      "1260        -1          96.0           16.166667          66.0    0.028037   \n",
      "1543        -1         270.0           18.066667         145.0    0.066890   \n",
      "969          1         175.0           17.600000         108.0    0.047619   \n",
      "1233        -1         273.0           17.125000         149.0    0.057239   \n",
      "650          1         116.0           14.625000          73.0    0.062992   \n",
      "1943        -1         219.0           10.000000         139.0    0.032787   \n",
      "1529        -1         253.0           28.222222         153.0    0.054152   \n",
      "414          1          98.0           14.142857          70.0    0.072072   \n",
      "1659        -1          65.0            4.400000          52.0    0.054795   \n",
      "671          1          52.0           13.250000          41.0    0.053571   \n",
      "1179         1         170.0           13.153846         104.0    0.061856   \n",
      "885          1         167.0           12.000000         105.0    0.016304   \n",
      "1198        -1         186.0           18.700000         107.0    0.084158   \n",
      "634          1         138.0           15.444444          85.0    0.079470   \n",
      "1898        -1         152.0           13.909091         103.0    0.035714   \n",
      "852          1          87.0           11.000000          63.0    0.010417   \n",
      "1201        -1          93.0           13.428571          63.0    0.030000   \n",
      "1828        -1         182.0           12.200000         117.0    0.067308   \n",
      "1870        -1         115.0            5.523810          83.0    0.021429   \n",
      "1432        -1         485.0           21.130435         240.0    0.064935   \n",
      "...        ...           ...                 ...           ...         ...   \n",
      "1593        -1         127.0           16.000000          94.0    0.082759   \n",
      "1083         1          86.0           17.400000          62.0    0.040404   \n",
      "748          1         109.0           15.714286          80.0    0.074380   \n",
      "1833        -1         122.0           13.666667          84.0    0.063830   \n",
      "486          1          84.0            8.500000          60.0    0.030612   \n",
      "1199        -1          60.0           10.166667          44.0    0.071429   \n",
      "556          1         236.0           11.285714         146.0    0.041667   \n",
      "1535        -1         212.0           21.300000         132.0    0.063348   \n",
      "1540        -1         110.0           15.857143          76.0    0.008197   \n",
      "1158         1         205.0           17.166667         120.0    0.066964   \n",
      "987          1         166.0           13.916667         108.0    0.043716   \n",
      "1863        -1         163.0            6.833333         111.0    0.040609   \n",
      "1266        -1         221.0           18.500000         131.0    0.059829   \n",
      "1893        -1         163.0           20.500000          94.0    0.077778   \n",
      "0            1          73.0           10.571429          53.0    0.073171   \n",
      "880          1          66.0           22.333333          57.0    0.037500   \n",
      "763          1          59.0            8.571429          39.0    0.042857   \n",
      "361          1         203.0           22.666667         119.0    0.058036   \n",
      "1509        -1         214.0           26.875000         128.0    0.064935   \n",
      "768          1          33.0            6.800000          29.0    0.024390   \n",
      "115          1         241.0           20.166667         154.0    0.048689   \n",
      "1715        -1         194.0           19.500000         120.0    0.037736   \n",
      "336          1          94.0            9.500000          64.0    0.046296   \n",
      "982          1         103.0           13.000000          71.0    0.071429   \n",
      "1637        -1         181.0            6.066667         109.0    0.051546   \n",
      "201          1          70.0            8.875000          48.0    0.024390   \n",
      "400          1          93.0           15.666667          76.0    0.028302   \n",
      "30           1          78.0           11.285714          56.0    0.022727   \n",
      "1682        -1         234.0           16.785714         151.0    0.023166   \n",
      "1806        -1         159.0            8.421053         103.0    0.040000   \n",
      "\n",
      "      brand  avg_word_length  connectors  digits  verbs_per_noun   adj  prep  \\\n",
      "1594    0.0         3.757794         8.0     3.0             1.0  14.0  46.0   \n",
      "1759    0.0         4.124031         3.0     1.0             1.0   5.0  14.0   \n",
      "713     0.0         4.451220         3.0     4.0             1.0   9.0   9.0   \n",
      "108     0.0         3.950000         3.0     0.0             1.0   5.0   5.0   \n",
      "348     0.0         3.630769         7.0     0.0             1.0   7.0  16.0   \n",
      "87      0.0         4.065693         5.0     2.0             1.0  13.0   5.0   \n",
      "1283    0.0         3.924460        10.0     2.0             1.0  27.0  23.0   \n",
      "700     0.0         4.462687         6.0     0.0             1.0  13.0  10.0   \n",
      "1250    0.0         4.263889         5.0     0.0             1.0  14.0  12.0   \n",
      "223     0.0         4.568182         2.0     0.0             1.0   8.0   2.0   \n",
      "1260    0.0         4.018692         4.0     0.0             1.0   7.0   6.0   \n",
      "1543    0.0         3.846154         5.0     0.0             1.0  16.0  35.0   \n",
      "969     0.0         4.365079        10.0     0.0             1.0  18.0  13.0   \n",
      "1233    0.0         3.882155        14.0     2.0             1.0  12.0  35.0   \n",
      "650     0.0         4.204724        10.0     0.0             1.0   8.0  10.0   \n",
      "1943    1.0         3.819672         8.0     0.0             1.0  23.0  14.0   \n",
      "1529    0.0         3.927798        12.0     4.0             1.0  23.0  28.0   \n",
      "414     0.0         3.837838         6.0     0.0             1.0   8.0  12.0   \n",
      "1659    0.0         3.589041         0.0     1.0             1.0   0.0   6.0   \n",
      "671     0.0         3.964286         2.0     0.0             1.0   3.0   4.0   \n",
      "1179    0.0         3.623711         6.0     3.0             1.0  20.0  21.0   \n",
      "885     1.0         3.576087         3.0     7.0             1.0  15.0   8.0   \n",
      "1198    0.0         4.277228         6.0     3.0             1.0   8.0  23.0   \n",
      "634     0.0         4.000000         7.0     0.0             1.0   7.0  15.0   \n",
      "1898    0.0         3.809524         6.0     6.0             1.0   5.0  14.0   \n",
      "852     0.0         4.281250         5.0     0.0             1.0   7.0   3.0   \n",
      "1201    0.0         3.980000         2.0     0.0             1.0  11.0   6.0   \n",
      "1828    0.0         3.706731         3.0     1.0             1.0  14.0  18.0   \n",
      "1870    0.0         3.500000         3.0     0.0             1.0  12.0   6.0   \n",
      "1432    0.0         3.821892        17.0     1.0             1.0  35.0  59.0   \n",
      "...     ...              ...         ...     ...             ...   ...   ...   \n",
      "1593    0.0         4.241379         6.0     5.0             1.0   9.0  16.0   \n",
      "1083    0.0         3.848485         2.0     1.0             1.0  10.0  10.0   \n",
      "748     0.0         4.289256         4.0     0.0             1.0   8.0  11.0   \n",
      "1833    0.0         3.836879         3.0    16.0             1.0   7.0  13.0   \n",
      "486     0.0         3.887755         2.0     0.0             1.0  10.0   7.0   \n",
      "1199    0.0         3.657143         1.0     0.0             1.0   8.0   4.0   \n",
      "556     0.0         4.231061         6.0     0.0             1.0  24.0  20.0   \n",
      "1535    0.0         4.266968         7.0     8.0             1.0  16.0  23.0   \n",
      "1540    0.0         4.073770         6.0     0.0             1.0   6.0   4.0   \n",
      "1158    0.0         3.986607         7.0     5.0             1.0  24.0  20.0   \n",
      "987     0.0         4.021858         3.0    18.0             1.0  11.0  14.0   \n",
      "1863    0.0         3.852792         3.0    12.0             1.0  17.0  10.0   \n",
      "1266    0.0         4.209402         9.0     2.0             1.0  16.0  20.0   \n",
      "1893    0.0         3.761111         4.0     0.0             1.0   3.0  18.0   \n",
      "0       0.0         4.451220         3.0     4.0             1.0   9.0   9.0   \n",
      "880     0.0         4.100000         5.0     3.0             1.0  10.0   5.0   \n",
      "763     0.0         3.871429         5.0     0.0             1.0  14.0   2.0   \n",
      "361     0.0         4.388393        16.0     3.0             1.0  22.0  17.0   \n",
      "1509    0.0         3.961039         3.0     0.0             1.0  12.0  23.0   \n",
      "768     0.0         3.731707         1.0     0.0             1.0   3.0   1.0   \n",
      "115     0.0         3.958801         7.0     0.0             1.0  11.0  26.0   \n",
      "1715    0.0         3.613208         8.0    16.0             1.0  12.0  24.0   \n",
      "336     0.0         4.222222         3.0     0.0             1.0  12.0   9.0   \n",
      "982     0.0         3.901786         5.0     5.0             1.0   8.0  14.0   \n",
      "1637    0.0         3.969072         7.0     2.0             1.0   5.0  14.0   \n",
      "201     0.0         4.219512         6.0     0.0             1.0   9.0   3.0   \n",
      "400     0.0         4.433962         5.0     0.0             1.0  10.0   6.0   \n",
      "30      0.0         4.659091         3.0     0.0             1.0  11.0   2.0   \n",
      "1682    0.0         4.034749         4.0     5.0             1.0  21.0  16.0   \n",
      "1806    0.0         3.868571         5.0     8.0             1.0   9.0  14.0   \n",
      "\n",
      "      adverb  \n",
      "1594    22.0  \n",
      "1759     7.0  \n",
      "713      3.0  \n",
      "108      6.0  \n",
      "348      6.0  \n",
      "87       6.0  \n",
      "1283    25.0  \n",
      "700      7.0  \n",
      "1250    12.0  \n",
      "223      2.0  \n",
      "1260     1.0  \n",
      "1543    21.0  \n",
      "969     25.0  \n",
      "1233    13.0  \n",
      "650     11.0  \n",
      "1943    11.0  \n",
      "1529    14.0  \n",
      "414      6.0  \n",
      "1659     4.0  \n",
      "671      8.0  \n",
      "1179     6.0  \n",
      "885      6.0  \n",
      "1198    10.0  \n",
      "634      7.0  \n",
      "1898    11.0  \n",
      "852      3.0  \n",
      "1201     6.0  \n",
      "1828    12.0  \n",
      "1870     9.0  \n",
      "1432    29.0  \n",
      "...      ...  \n",
      "1593    10.0  \n",
      "1083     8.0  \n",
      "748      8.0  \n",
      "1833     6.0  \n",
      "486     13.0  \n",
      "1199    11.0  \n",
      "556     14.0  \n",
      "1535    19.0  \n",
      "1540     6.0  \n",
      "1158    15.0  \n",
      "987     14.0  \n",
      "1863     8.0  \n",
      "1266    19.0  \n",
      "1893    13.0  \n",
      "0        3.0  \n",
      "880      7.0  \n",
      "763      9.0  \n",
      "361      7.0  \n",
      "1509    17.0  \n",
      "768      5.0  \n",
      "115     15.0  \n",
      "1715    18.0  \n",
      "336      9.0  \n",
      "982      5.0  \n",
      "1637    14.0  \n",
      "201      8.0  \n",
      "400      5.0  \n",
      "30       6.0  \n",
      "1682    13.0  \n",
      "1806     7.0  \n",
      "\n",
      "[396 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'spam'], \n",
    "                                                    df['spam'], test_size=0.2,random_state=1)\n",
    "print(X_train.shape, X_test.shape,y_train.shape)\n",
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "print(type(y_test))\n",
    "print(type(X_test))\n",
    "print(X_test.shape)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transforming review text data to numerical form using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(min_df=4, max_df=0.7, sublinear_tf=True, use_idf=True,stop_words='english')\n",
    "#train_tfidf = vectorizer.fit_transform(X_train['review']).todense()\n",
    "#test_tfidf = vectorizer.transform(X_test['review']).todense()\n",
    "#print(train_tfidf.shape)\n",
    "#print(X_train.shape)\n",
    "#X_train.drop(['review'],axis=1,inplace=True)\n",
    "#X_test.drop(['review'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_tfidf.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tfidf = pd.DataFrame(train_tfidf)\n",
    "#print(type(X_train))\n",
    "#print(type(train_tfidf))\n",
    "#test_tfidf = pd.DataFrame(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.concat([X_train,train_tfidf],axis=1)\n",
    "#X_test  = pd.concat([X_test ,test_tfidf ],axis=1)\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Co-relational Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "numerical = list(set(df.columns) - \n",
    "                 set(['review']))\n",
    "# Calculate and plot\n",
    "corr_matrix = df[numerical].corr()\n",
    "sns.heatmap(corr_matrix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-10a16528b51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel':('linear','rbf'), 'C':[1,10]}\n",
    "clf=GridSearchCV(svc,parameters,cv=10)\n",
    "model = clf.fit(X_train,y_train.values.ravel())\n",
    "print (\"Score:\", model.score(X_test, y_test.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6540404040404041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model2 = GaussianNB()\n",
    "model2.fit(X_train,y_train)\n",
    "print (\"Score:\", model2.score(X_test, y_test.values.ravel()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model2, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      polarity  num_of_words  avg_words_per_sent  unique_words  self_words  \\\n",
      "1594        -1         376.0           11.424242         192.0    0.055156   \n",
      "1759        -1         119.0           17.142857          72.0    0.038760   \n",
      "713          1          73.0           10.571429          53.0    0.073171   \n",
      "108          1          54.0            9.166667          35.0    0.066667   \n",
      "348          1         121.0           17.428571          72.0    0.053846   \n",
      "87           1         123.0           17.714286          88.0    0.036496   \n",
      "1283        -1         250.0           15.687500         153.0    0.043165   \n",
      "700          1         118.0           11.900000          74.0    0.044776   \n",
      "1250        -1         128.0           14.333333          94.0    0.062500   \n",
      "223          1          39.0            8.000000          33.0    0.045455   \n",
      "1260        -1          96.0           16.166667          66.0    0.028037   \n",
      "1543        -1         270.0           18.066667         145.0    0.066890   \n",
      "969          1         175.0           17.600000         108.0    0.047619   \n",
      "1233        -1         273.0           17.125000         149.0    0.057239   \n",
      "650          1         116.0           14.625000          73.0    0.062992   \n",
      "1943        -1         219.0           10.000000         139.0    0.032787   \n",
      "1529        -1         253.0           28.222222         153.0    0.054152   \n",
      "414          1          98.0           14.142857          70.0    0.072072   \n",
      "1659        -1          65.0            4.400000          52.0    0.054795   \n",
      "671          1          52.0           13.250000          41.0    0.053571   \n",
      "1179         1         170.0           13.153846         104.0    0.061856   \n",
      "885          1         167.0           12.000000         105.0    0.016304   \n",
      "1198        -1         186.0           18.700000         107.0    0.084158   \n",
      "634          1         138.0           15.444444          85.0    0.079470   \n",
      "1898        -1         152.0           13.909091         103.0    0.035714   \n",
      "852          1          87.0           11.000000          63.0    0.010417   \n",
      "1201        -1          93.0           13.428571          63.0    0.030000   \n",
      "1828        -1         182.0           12.200000         117.0    0.067308   \n",
      "1870        -1         115.0            5.523810          83.0    0.021429   \n",
      "1432        -1         485.0           21.130435         240.0    0.064935   \n",
      "...        ...           ...                 ...           ...         ...   \n",
      "1593        -1         127.0           16.000000          94.0    0.082759   \n",
      "1083         1          86.0           17.400000          62.0    0.040404   \n",
      "748          1         109.0           15.714286          80.0    0.074380   \n",
      "1833        -1         122.0           13.666667          84.0    0.063830   \n",
      "486          1          84.0            8.500000          60.0    0.030612   \n",
      "1199        -1          60.0           10.166667          44.0    0.071429   \n",
      "556          1         236.0           11.285714         146.0    0.041667   \n",
      "1535        -1         212.0           21.300000         132.0    0.063348   \n",
      "1540        -1         110.0           15.857143          76.0    0.008197   \n",
      "1158         1         205.0           17.166667         120.0    0.066964   \n",
      "987          1         166.0           13.916667         108.0    0.043716   \n",
      "1863        -1         163.0            6.833333         111.0    0.040609   \n",
      "1266        -1         221.0           18.500000         131.0    0.059829   \n",
      "1893        -1         163.0           20.500000          94.0    0.077778   \n",
      "0            1          73.0           10.571429          53.0    0.073171   \n",
      "880          1          66.0           22.333333          57.0    0.037500   \n",
      "763          1          59.0            8.571429          39.0    0.042857   \n",
      "361          1         203.0           22.666667         119.0    0.058036   \n",
      "1509        -1         214.0           26.875000         128.0    0.064935   \n",
      "768          1          33.0            6.800000          29.0    0.024390   \n",
      "115          1         241.0           20.166667         154.0    0.048689   \n",
      "1715        -1         194.0           19.500000         120.0    0.037736   \n",
      "336          1          94.0            9.500000          64.0    0.046296   \n",
      "982          1         103.0           13.000000          71.0    0.071429   \n",
      "1637        -1         181.0            6.066667         109.0    0.051546   \n",
      "201          1          70.0            8.875000          48.0    0.024390   \n",
      "400          1          93.0           15.666667          76.0    0.028302   \n",
      "30           1          78.0           11.285714          56.0    0.022727   \n",
      "1682        -1         234.0           16.785714         151.0    0.023166   \n",
      "1806        -1         159.0            8.421053         103.0    0.040000   \n",
      "\n",
      "      brand  avg_word_length  connectors  digits  verbs_per_noun   adj  prep  \\\n",
      "1594    0.0         3.757794         8.0     3.0             1.0  14.0  46.0   \n",
      "1759    0.0         4.124031         3.0     1.0             1.0   5.0  14.0   \n",
      "713     0.0         4.451220         3.0     4.0             1.0   9.0   9.0   \n",
      "108     0.0         3.950000         3.0     0.0             1.0   5.0   5.0   \n",
      "348     0.0         3.630769         7.0     0.0             1.0   7.0  16.0   \n",
      "87      0.0         4.065693         5.0     2.0             1.0  13.0   5.0   \n",
      "1283    0.0         3.924460        10.0     2.0             1.0  27.0  23.0   \n",
      "700     0.0         4.462687         6.0     0.0             1.0  13.0  10.0   \n",
      "1250    0.0         4.263889         5.0     0.0             1.0  14.0  12.0   \n",
      "223     0.0         4.568182         2.0     0.0             1.0   8.0   2.0   \n",
      "1260    0.0         4.018692         4.0     0.0             1.0   7.0   6.0   \n",
      "1543    0.0         3.846154         5.0     0.0             1.0  16.0  35.0   \n",
      "969     0.0         4.365079        10.0     0.0             1.0  18.0  13.0   \n",
      "1233    0.0         3.882155        14.0     2.0             1.0  12.0  35.0   \n",
      "650     0.0         4.204724        10.0     0.0             1.0   8.0  10.0   \n",
      "1943    1.0         3.819672         8.0     0.0             1.0  23.0  14.0   \n",
      "1529    0.0         3.927798        12.0     4.0             1.0  23.0  28.0   \n",
      "414     0.0         3.837838         6.0     0.0             1.0   8.0  12.0   \n",
      "1659    0.0         3.589041         0.0     1.0             1.0   0.0   6.0   \n",
      "671     0.0         3.964286         2.0     0.0             1.0   3.0   4.0   \n",
      "1179    0.0         3.623711         6.0     3.0             1.0  20.0  21.0   \n",
      "885     1.0         3.576087         3.0     7.0             1.0  15.0   8.0   \n",
      "1198    0.0         4.277228         6.0     3.0             1.0   8.0  23.0   \n",
      "634     0.0         4.000000         7.0     0.0             1.0   7.0  15.0   \n",
      "1898    0.0         3.809524         6.0     6.0             1.0   5.0  14.0   \n",
      "852     0.0         4.281250         5.0     0.0             1.0   7.0   3.0   \n",
      "1201    0.0         3.980000         2.0     0.0             1.0  11.0   6.0   \n",
      "1828    0.0         3.706731         3.0     1.0             1.0  14.0  18.0   \n",
      "1870    0.0         3.500000         3.0     0.0             1.0  12.0   6.0   \n",
      "1432    0.0         3.821892        17.0     1.0             1.0  35.0  59.0   \n",
      "...     ...              ...         ...     ...             ...   ...   ...   \n",
      "1593    0.0         4.241379         6.0     5.0             1.0   9.0  16.0   \n",
      "1083    0.0         3.848485         2.0     1.0             1.0  10.0  10.0   \n",
      "748     0.0         4.289256         4.0     0.0             1.0   8.0  11.0   \n",
      "1833    0.0         3.836879         3.0    16.0             1.0   7.0  13.0   \n",
      "486     0.0         3.887755         2.0     0.0             1.0  10.0   7.0   \n",
      "1199    0.0         3.657143         1.0     0.0             1.0   8.0   4.0   \n",
      "556     0.0         4.231061         6.0     0.0             1.0  24.0  20.0   \n",
      "1535    0.0         4.266968         7.0     8.0             1.0  16.0  23.0   \n",
      "1540    0.0         4.073770         6.0     0.0             1.0   6.0   4.0   \n",
      "1158    0.0         3.986607         7.0     5.0             1.0  24.0  20.0   \n",
      "987     0.0         4.021858         3.0    18.0             1.0  11.0  14.0   \n",
      "1863    0.0         3.852792         3.0    12.0             1.0  17.0  10.0   \n",
      "1266    0.0         4.209402         9.0     2.0             1.0  16.0  20.0   \n",
      "1893    0.0         3.761111         4.0     0.0             1.0   3.0  18.0   \n",
      "0       0.0         4.451220         3.0     4.0             1.0   9.0   9.0   \n",
      "880     0.0         4.100000         5.0     3.0             1.0  10.0   5.0   \n",
      "763     0.0         3.871429         5.0     0.0             1.0  14.0   2.0   \n",
      "361     0.0         4.388393        16.0     3.0             1.0  22.0  17.0   \n",
      "1509    0.0         3.961039         3.0     0.0             1.0  12.0  23.0   \n",
      "768     0.0         3.731707         1.0     0.0             1.0   3.0   1.0   \n",
      "115     0.0         3.958801         7.0     0.0             1.0  11.0  26.0   \n",
      "1715    0.0         3.613208         8.0    16.0             1.0  12.0  24.0   \n",
      "336     0.0         4.222222         3.0     0.0             1.0  12.0   9.0   \n",
      "982     0.0         3.901786         5.0     5.0             1.0   8.0  14.0   \n",
      "1637    0.0         3.969072         7.0     2.0             1.0   5.0  14.0   \n",
      "201     0.0         4.219512         6.0     0.0             1.0   9.0   3.0   \n",
      "400     0.0         4.433962         5.0     0.0             1.0  10.0   6.0   \n",
      "30      0.0         4.659091         3.0     0.0             1.0  11.0   2.0   \n",
      "1682    0.0         4.034749         4.0     5.0             1.0  21.0  16.0   \n",
      "1806    0.0         3.868571         5.0     8.0             1.0   9.0  14.0   \n",
      "\n",
      "      adverb  \n",
      "1594    22.0  \n",
      "1759     7.0  \n",
      "713      3.0  \n",
      "108      6.0  \n",
      "348      6.0  \n",
      "87       6.0  \n",
      "1283    25.0  \n",
      "700      7.0  \n",
      "1250    12.0  \n",
      "223      2.0  \n",
      "1260     1.0  \n",
      "1543    21.0  \n",
      "969     25.0  \n",
      "1233    13.0  \n",
      "650     11.0  \n",
      "1943    11.0  \n",
      "1529    14.0  \n",
      "414      6.0  \n",
      "1659     4.0  \n",
      "671      8.0  \n",
      "1179     6.0  \n",
      "885      6.0  \n",
      "1198    10.0  \n",
      "634      7.0  \n",
      "1898    11.0  \n",
      "852      3.0  \n",
      "1201     6.0  \n",
      "1828    12.0  \n",
      "1870     9.0  \n",
      "1432    29.0  \n",
      "...      ...  \n",
      "1593    10.0  \n",
      "1083     8.0  \n",
      "748      8.0  \n",
      "1833     6.0  \n",
      "486     13.0  \n",
      "1199    11.0  \n",
      "556     14.0  \n",
      "1535    19.0  \n",
      "1540     6.0  \n",
      "1158    15.0  \n",
      "987     14.0  \n",
      "1863     8.0  \n",
      "1266    19.0  \n",
      "1893    13.0  \n",
      "0        3.0  \n",
      "880      7.0  \n",
      "763      9.0  \n",
      "361      7.0  \n",
      "1509    17.0  \n",
      "768      5.0  \n",
      "115     15.0  \n",
      "1715    18.0  \n",
      "336      9.0  \n",
      "982      5.0  \n",
      "1637    14.0  \n",
      "201      8.0  \n",
      "400      5.0  \n",
      "30       6.0  \n",
      "1682    13.0  \n",
      "1806     7.0  \n",
      "\n",
      "[396 rows x 13 columns]\n",
      "[0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
      " 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
      " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(model2.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
