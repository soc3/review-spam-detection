{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import features\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mess):\n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/TrainingSet\") as fh:\n",
    "    data = json.load(fh)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"review_id\", \"hotel_name\", \"review\", \"polarity\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "df[\"num_of_words\"] = np.nan\n",
    "df[\"avg_words_per_sent\"]=np.nan\n",
    "df[\"unique_words\"]=np.nan\n",
    "df[\"self_words\"]=np.nan\n",
    "df[\"brand\"]=np.nan\n",
    "df[\"avg_word_length\"]=np.nan\n",
    "df[\"connectors\"]=np.nan\n",
    "df[\"digits\"]=np.nan\n",
    "df[\"verbs_per_noun\"]=np.nan\n",
    "df[\"adj\"]=np.nan\n",
    "df[\"prep\"]=np.nan\n",
    "df[\"adverb\"]=np.nan\n",
    "for f in df[\"review\"]:\n",
    "    a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12 = features.majorfunc(f)\n",
    "    df.iloc[i, df.columns.get_loc('num_of_words')] = a1\n",
    "    df.iloc[i, df.columns.get_loc('avg_words_per_sent')] = a2\n",
    "    df.iloc[i, df.columns.get_loc('unique_words')] = a3\n",
    "    df.iloc[i, df.columns.get_loc('self_words')] = a4\n",
    "    df.iloc[i, df.columns.get_loc('brand')] = a5\n",
    "    df.iloc[i, df.columns.get_loc('avg_word_length')] = a6\n",
    "    df.iloc[i, df.columns.get_loc('connectors')] = a7\n",
    "    df.iloc[i, df.columns.get_loc('digits')] = a8\n",
    "    df.iloc[i, df.columns.get_loc('verbs_per_noun')] = a9\n",
    "    df.iloc[i, df.columns.get_loc('adj')] = a10\n",
    "    df.iloc[i, df.columns.get_loc('prep')] = a11\n",
    "    df.iloc[i, df.columns.get_loc('adverb')] = a12\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5\n",
      "1        4\n",
      "2       15\n",
      "3        7\n",
      "4        9\n",
      "5        2\n",
      "6        3\n",
      "7       13\n",
      "8       19\n",
      "9       17\n",
      "10      18\n",
      "11      12\n",
      "12       4\n",
      "13       8\n",
      "14      18\n",
      "15       7\n",
      "16      11\n",
      "17      16\n",
      "18      16\n",
      "19      16\n",
      "20       6\n",
      "21       8\n",
      "22      10\n",
      "23      13\n",
      "24       6\n",
      "25      18\n",
      "26       0\n",
      "27       9\n",
      "28      11\n",
      "29      17\n",
      "        ..\n",
      "1950    19\n",
      "1951    17\n",
      "1952    18\n",
      "1953    16\n",
      "1954    12\n",
      "1955    14\n",
      "1956    18\n",
      "1957     1\n",
      "1958     3\n",
      "1959     1\n",
      "1960    17\n",
      "1961     7\n",
      "1962    13\n",
      "1963     8\n",
      "1964     5\n",
      "1965    17\n",
      "1966    13\n",
      "1967     4\n",
      "1968    15\n",
      "1969    12\n",
      "1970    15\n",
      "1971    17\n",
      "1972     9\n",
      "1973     9\n",
      "1974    13\n",
      "1975    11\n",
      "1976    14\n",
      "1977    15\n",
      "1978     2\n",
      "1979     4\n",
      "Name: hotel_name, Length: 1980, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[\"hotel_name\"] = le.fit_transform(df[\"hotel_name\"].astype('str'))\n",
    "# print(df[\"hotel_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_dummies = pd.get_dummies(df['hotel_name']) # one hot encoding\n",
    "\n",
    "df = pd.concat([df, just_dummies], axis=1)      \n",
    "df.drop(['hotel_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['review_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  polarity  spam  \\\n",
      "0  Fairmont Chicago was a great choice for my wif...         1     1   \n",
      "1  Conrad Chicago it was 5:00 AM my plan just fle...         1     1   \n",
      "2  My husband and I snagged a great deal on a wee...         1     1   \n",
      "3  The Hilton in Chicago was awesome. The room wa...         1     1   \n",
      "4  My husband and I stayed at the Hyatt Regency w...         1     1   \n",
      "\n",
      "   num_of_words  avg_words_per_sent  unique_words  self_words  brand  \\\n",
      "0          73.0           10.571429          53.0    0.073171    0.0   \n",
      "1         351.0           23.466667         181.0    0.074380    0.0   \n",
      "2         135.0           17.000000          94.0    0.046667    0.0   \n",
      "3         104.0            8.750000          61.0    0.058824    0.0   \n",
      "4         114.0           14.375000          80.0    0.072581    0.0   \n",
      "\n",
      "   avg_word_length  connectors ...  10  11  12  13  14  15  16  17  18  19  \n",
      "0         4.451220         3.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "1         3.730028        12.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "2         4.066667         4.0 ...   0   0   0   0   0   1   0   0   0   0  \n",
      "3         3.781513         5.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "4         4.145161         3.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([            'review',           'polarity',               'spam',\n",
      "             'num_of_words', 'avg_words_per_sent',       'unique_words',\n",
      "               'self_words',              'brand',    'avg_word_length',\n",
      "               'connectors',             'digits',     'verbs_per_noun',\n",
      "                      'adj',               'prep',             'adverb',\n",
      "                          0,                    1,                    2,\n",
      "                          3,                    4,                    5,\n",
      "                          6,                    7,                    8,\n",
      "                          9,                   10,                   11,\n",
      "                         12,                   13,                   14,\n",
      "                         15,                   16,                   17,\n",
      "                         18,                   19],\n",
      "      dtype='object')\n",
      "Index([            'review',           'polarity',               'spam',\n",
      "             'num_of_words', 'avg_words_per_sent',       'unique_words',\n",
      "               'self_words',              'brand',    'avg_word_length',\n",
      "               'connectors',             'digits',     'verbs_per_noun',\n",
      "                      'adj',               'prep',             'adverb',\n",
      "                       1000,                 1001,                 1002,\n",
      "                       1003,                 1004,                 1005,\n",
      "                       1006,                 1007,                 1008,\n",
      "                       1009,                 1010,                 1011,\n",
      "                       1012,                 1013,                 1014,\n",
      "                       1015,                 1016,                 1017,\n",
      "                       1018,                 1019],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Rename hotword encoding columns\n",
    "print(df.columns)\n",
    "dic = {}\n",
    "for i in range(20):\n",
    "    dic[i]=i+1000\n",
    "df.rename(columns=dic,inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 500)\n",
      "(1980, 35)\n",
      "(1980, 535)\n"
     ]
    }
   ],
   "source": [
    "# Using HashingVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "# text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=500)\n",
    "# encode document\n",
    "vector = vectorizer.transform(df['review'])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "df1 = pd.DataFrame(vector.toarray())\n",
    "print(df.shape)\n",
    "df = pd.concat([df,df1],axis=1)\n",
    "print(df.shape)\n",
    "df.drop(['review'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 34) (396, 34)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'spam'], df['spam'], test_size=0.2,random_state=1)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "# text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.7, sublinear_tf=True, use_idf=True,stop_words='english')\n",
    "train_tfidf = vectorizer.fit_transform(X_train['review'])\n",
    "test_tfidf = vectorizer.transform(X_test['review'])\n",
    "# # encode document\n",
    "# vector = vectorizer.transform(df['review'])\n",
    "# # summarize encoded vector\n",
    "# print(vector.shape)\n",
    "# df1 = pd.DataFrame(vector.toarray())\n",
    "# print(df.shape)\n",
    "# df = pd.concat([df,df1],axis=1)\n",
    "# print(df.shape)\n",
    "# df.drop(['review'],axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9217171717171717\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df.loc[:,df.columns!='spam'],df[\"spam\"], random_state=1,test_size=0.1)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel':('linear','rbf'), 'C':[1,10]}\n",
    "clf=GridSearchCV(svc,parameters,cv=10)\n",
    "model = clf.fit(train_tfidf,y_train)\n",
    "print (\"Score:\", model.score(test_tfidf, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8585858585858586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "model1 = bnb.fit(train_tfidf,y_train)\n",
    "print (\"Score:\", model1.score(test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.851010101010101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "model2 = lr.fit(train_tfidf,y_train)\n",
    "print (\"Score:\", model2.score(test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df.loc[:,df.columns!='spam'],df[\"spam\"], random_state=1,test_size=0.1)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel':('linear','rbf'), 'C':[1,10]}\n",
    "clf=GridSearchCV(svc,parameters,cv=10)\n",
    "model = clf.fit(X_train,y_train)\n",
    "print (\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "train_dtm = vect.fit_transform(X_train[\"review\"])\n",
    "test_dtm = vect.transform(X_test[\"review\"])\n",
    "\n",
    "print(type(train_dtm))\n",
    "'''\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "y_pred_class = nb.predict(test_dtm)\n",
    "'''\n",
    "\n",
    "#text_mnb_stemmed = Pipeline([('tfidf', TfidfTransformer()),('mnb', MultinomialNB(fit_prior=False)),])\n",
    "\n",
    "#text_mnb_stemmed = text_mnb_stemmed.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Data/TestSet\") as fh:\n",
    "    data1 = json.load(fh)\n",
    "dfd = pd.DataFrame(data1)\n",
    "dfd.columns = [\"review_id\", \"hotel_name\", \"review\", \"polarity\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df[\"num_of_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
