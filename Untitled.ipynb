{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import features\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(mess):\n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Data/TrainingSet\") as fh:\n",
    "    data = json.load(fh)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"review_id\", \"hotel_name\", \"review\", \"polarity\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "df[\"num_of_words\"] = np.nan\n",
    "df[\"avg_words_per_sent\"]=np.nan\n",
    "df[\"unique_words\"]=np.nan\n",
    "df[\"self_words\"]=np.nan\n",
    "df[\"brand\"]=np.nan\n",
    "df[\"avg_word_length\"]=np.nan\n",
    "df[\"connectors\"]=np.nan\n",
    "df[\"digits\"]=np.nan\n",
    "df[\"verbs_per_noun\"]=np.nan\n",
    "df[\"adj\"]=np.nan\n",
    "df[\"prep\"]=np.nan\n",
    "df[\"adverb\"]=np.nan\n",
    "for f in df[\"review\"]:\n",
    "    a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12 = features.majorfunc(f)\n",
    "    df.iloc[i, df.columns.get_loc('num_of_words')] = a1\n",
    "    df.iloc[i, df.columns.get_loc('avg_words_per_sent')] = a2\n",
    "    df.iloc[i, df.columns.get_loc('unique_words')] = a3\n",
    "    df.iloc[i, df.columns.get_loc('self_words')] = a4\n",
    "    df.iloc[i, df.columns.get_loc('brand')] = a5\n",
    "    df.iloc[i, df.columns.get_loc('avg_word_length')] = a6\n",
    "    df.iloc[i, df.columns.get_loc('connectors')] = a7\n",
    "    df.iloc[i, df.columns.get_loc('digits')] = a8\n",
    "    df.iloc[i, df.columns.get_loc('verbs_per_noun')] = a9\n",
    "    df.iloc[i, df.columns.get_loc('adj')] = a10\n",
    "    df.iloc[i, df.columns.get_loc('prep')] = a11\n",
    "    df.iloc[i, df.columns.get_loc('adverb')] = a12\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        5\n",
      "1        4\n",
      "2       15\n",
      "3        7\n",
      "4        9\n",
      "5        2\n",
      "6        3\n",
      "7       13\n",
      "8       19\n",
      "9       17\n",
      "10      18\n",
      "11      12\n",
      "12       4\n",
      "13       8\n",
      "14      18\n",
      "15       7\n",
      "16      11\n",
      "17      16\n",
      "18      16\n",
      "19      16\n",
      "20       6\n",
      "21       8\n",
      "22      10\n",
      "23      13\n",
      "24       6\n",
      "25      18\n",
      "26       0\n",
      "27       9\n",
      "28      11\n",
      "29      17\n",
      "        ..\n",
      "1950    19\n",
      "1951    17\n",
      "1952    18\n",
      "1953    16\n",
      "1954    12\n",
      "1955    14\n",
      "1956    18\n",
      "1957     1\n",
      "1958     3\n",
      "1959     1\n",
      "1960    17\n",
      "1961     7\n",
      "1962    13\n",
      "1963     8\n",
      "1964     5\n",
      "1965    17\n",
      "1966    13\n",
      "1967     4\n",
      "1968    15\n",
      "1969    12\n",
      "1970    15\n",
      "1971    17\n",
      "1972     9\n",
      "1973     9\n",
      "1974    13\n",
      "1975    11\n",
      "1976    14\n",
      "1977    15\n",
      "1978     2\n",
      "1979     4\n",
      "Name: hotel_name, Length: 1980, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[\"hotel_name\"] = le.fit_transform(df[\"hotel_name\"].astype('str'))\n",
    "print(df[\"hotel_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "just_dummies = pd.get_dummies(df['hotel_name']) # one hot encoding\n",
    "\n",
    "df = pd.concat([df, just_dummies], axis=1)      \n",
    "df.drop(['hotel_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['review_id'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-df2477bdfd40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3624\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['review_id'] not contained in axis"
     ]
    }
   ],
   "source": [
    "df.drop(['review_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  polarity  spam  \\\n",
      "0  Fairmont Chicago was a great choice for my wif...         1     1   \n",
      "1  Conrad Chicago it was 5:00 AM my plan just fle...         1     1   \n",
      "2  My husband and I snagged a great deal on a wee...         1     1   \n",
      "3  The Hilton in Chicago was awesome. The room wa...         1     1   \n",
      "4  My husband and I stayed at the Hyatt Regency w...         1     1   \n",
      "\n",
      "   num_of_words  avg_words_per_sent  unique_words  self_words  brand  \\\n",
      "0          73.0           10.571429          53.0    0.073171    0.0   \n",
      "1         351.0           23.466667         181.0    0.074380    0.0   \n",
      "2         135.0           17.000000          94.0    0.046667    0.0   \n",
      "3         104.0            8.750000          61.0    0.058824    0.0   \n",
      "4         114.0           14.375000          80.0    0.072581    0.0   \n",
      "\n",
      "   avg_word_length  connectors ...  10  11  12  13  14  15  16  17  18  19  \n",
      "0         4.451220         3.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "1         3.730028        12.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "2         4.066667         4.0 ...   0   0   0   0   0   1   0   0   0   0  \n",
      "3         3.781513         5.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "4         4.145161         3.0 ...   0   0   0   0   0   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['review'] = data['review'].head(5).apply(preprocess)\n",
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "all_documents = df[\"review\"].tolist()\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    "\n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [word_tokenize(d) for d in documents]#[tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "#in Scikit-Learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "\n",
    "\n",
    "########### END BLOG POST 1 #############\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "tfidf_representation = tfidf(all_documents)\n",
    "our_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(tfidf_representation):\n",
    "    for count_1, doc_1 in enumerate(tfidf_representation):\n",
    "        our_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "skl_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(sklearn_representation.toarray()):\n",
    "    for count_1, doc_1 in enumerate(sklearn_representation.toarray()):\n",
    "        skl_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"rev1\"] = np.nan\n",
    "df[\"rev2\"]=np.nan\n",
    "df[\"rev3\"]=np.nan\n",
    "df[\"rev4\"]=np.nan\n",
    "df[\"rev5\"]=np.nan\n",
    "df[\"rev6\"]=np.nan\n",
    "i=0\n",
    "for x in zip(sorted(our_tfidf_comparisons, reverse = True), sorted(skl_tfidf_comparisons, reverse = True)):\n",
    "    a,b = x\n",
    "    c,d,e = a\n",
    "    f,g,h = b\n",
    "    df.iloc[i, df.columns.get_loc('rev1')] = c\n",
    "    df.iloc[i, df.columns.get_loc('rev2')] = d\n",
    "    df.iloc[i, df.columns.get_loc('rev3')] = e\n",
    "    df.iloc[i, df.columns.get_loc('rev4')] = f\n",
    "    df.iloc[i, df.columns.get_loc('rev5')] = g\n",
    "    df.iloc[i, df.columns.get_loc('rev6')] = h\n",
    "df.drop(['review'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'spam'], df['spam'], test_size=0.2,random_state=1)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df.loc[:,df.columns!='spam'],df[\"spam\"], random_state=1,test_size=0.1)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel':('linear','rbf'), 'C':[1,10]}\n",
    "clf=GridSearchCV(svc,parameters,cv=10)\n",
    "model = clf.fit(X_train,y_train)\n",
    "print (\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "train_dtm = vect.fit_transform(X_train[\"review\"])\n",
    "test_dtm = vect.transform(X_test[\"review\"])\n",
    "\n",
    "print(type(train_dtm))\n",
    "'''\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_dtm, y_train)\n",
    "y_pred_class = nb.predict(test_dtm)\n",
    "'''\n",
    "\n",
    "#text_mnb_stemmed = Pipeline([('tfidf', TfidfTransformer()),('mnb', MultinomialNB(fit_prior=False)),])\n",
    "\n",
    "#text_mnb_stemmed = text_mnb_stemmed.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"Data/TestSet\") as fh:\n",
    "    data1 = json.load(fh)\n",
    "dfd = pd.DataFrame(data1)\n",
    "dfd.columns = [\"review_id\", \"hotel_name\", \"review\", \"polarity\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df[\"num_of_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
